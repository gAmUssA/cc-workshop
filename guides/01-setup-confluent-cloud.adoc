= üîê Setting Up Confluent Cloud (15 minutes)
Viktor Gamov <vgamov@confluent.io>, ¬© 2025 Confluent, Inc.
2025-09-11
:revdate: 2025-09-11
:linkattrs:
:ast: &ast;
:y: &#10003;
:n: &#10008;
:y: icon:check-sign[role="green"]
:n: icon:check-minus[role="red"]
:c: icon:file-text-alt[role="blue"]
:toc: auto
:toc-placement: auto
:toc-position: auto
:toc-title: Setup Guide Contents
:toclevels: 3
:idprefix:
:idseparator: -
:sectanchors:
:icons: font
:source-highlighter: highlight.js
:highlightjs-theme: idea
:experimental:

This guide walks you through setting up your Confluent Cloud environment for the workshop.

toc::[]

== üéØ Learning Objectives

By the end of this section, you will have:

* ‚úÖ Validated all prerequisite tools are installed
* ‚úÖ Authenticated with Confluent Cloud
* ‚úÖ Created a working Kafka cluster
* ‚úÖ Generated API keys for cluster access
* ‚úÖ Validated Tableflow access with existing credentials

== ‚è±Ô∏è Time Allocation

* **Prerequisites Validation**: 3 minutes
* **Authentication & Environment Setup**: 7 minutes  
* **Kafka Cluster Creation**: 5 minutes
* **Tableflow Validation**: 2 minutes

== üîç Prerequisites Validation (3 minutes)

=== Check VSCode Confluent Extension

. Open VSCode
. Go to Extensions (Ctrl+Shift+X / Cmd+Shift+X)
. Search for "Confluent"
. Verify the Confluent extension is installed and enabled

[source,bash]
----
# Alternative: Check if extension is installed via CLI
code --list-extensions | grep confluent
----

=== Verify Confluent CLI Installation

[source,bash]
----
# Check CLI installation and version
confluent version

# Expected output should show version 4.x or higher
# confluent - Confluent CLI
# Version:     v4.x.x
# Git Ref:     xxxxxxx
# Build Date:  2025-xx-xx
----

If not installed, run:
[source,bash]
----
# Install Confluent CLI
curl -sL --http1.1 https://cnfl.io/cli | sh -s -- latest
----

=== Validate DuckDB Installation

[source,bash]
----
# Check DuckDB installation
duckdb --version

# Expected output: v0.x.x or higher
----

If not installed, visit: https://duckdb.org/docs/installation/

== üåç Authentication & Environment Setup (7 minutes)

=== Login to Confluent Cloud

[source,bash]
----
# Login to Confluent Cloud
confluent login --save
----

=== Create or Select Organization

[source,bash]
----
# List available organizations
confluent organization list
----

=== Create Workshop Environment

[source,bash]
----
# Create a new environment for the workshop
confluent environment create "cc-workshop-env"

# List environments to see your new environment
confluent environment list

# Use the workshop environment (replace $CC_ENV_ID with actual ID)
# export CC_ENV_ID=<your-env-id>
confluent environment use $CC_ENV_ID
----

== ‚òÅÔ∏è Kafka Cluster Creation (5 minutes)

=== Create Basic Kafka Cluster

[source,bash]
----
# Create a Basic cluster (suitable for development/workshop)
confluent kafka cluster create workshop-cluster \
  --cloud aws \
  --region us-east-1 \
  --type basic
----

NOTE: Basic clusters are free and perfect for workshops.
They have some limitations but are sufficient for our use case

=== Configure Cluster Settings

[source,bash]
----
# List clusters to get cluster ID
confluent kafka cluster list

# Use the cluster (replace CLUSTER_ID with actual ID)
# export CC_KAFKA_CLUSTER=<your-cluster-id>
confluent kafka cluster use $CC_KAFKA_CLUSTER

# Describe cluster to verify settings
confluent kafka cluster describe $CC_KAFKA_CLUSTER
----

=== Generate API Keys

==== Kafka Cluster API Keys

[source,bash]
----
# Create API key for cluster access
confluent api-key create --resource $CC_KAFKA_CLUSTER --description "Workshop API Key for Kafka Cluster"

# Store the API key and secret - you'll need them later
# API Key: <your-kafka-api-key>
# API Secret: <your-kafka-api-secret>

# Use the API key
# export KAFKA_API_KEY=<your-kafka-api-key>
# export KAFKA_API_SECRET=<your-kafka-api-secret>
confluent api-key use $KAFKA_API_KEY --resource $CC_KAFKA_CLUSTER
----

==== Schema Registry API Keys

[source,bash]
----
# Get Schema Registry cluster ID
confluent schema-registry cluster describe

# Create API key for Schema Registry access
# export SCHEMA_REGISTRY_CLUSTER_ID=<your-sr-cluster-id>
confluent api-key create --resource $SCHEMA_REGISTRY_CLUSTER_ID --description "Workshop API Key for Schema Registry"

# Store the Schema Registry API key and secret
# Schema Registry API Key: <your-sr-api-key>
# Schema Registry API Secret: <your-sr-api-secret>

# Use the Schema Registry API key
# export SCHEMA_REGISTRY_API_KEY=<your-sr-api-key>
# export SCHEMA_REGISTRY_API_SECRET=<your-sr-api-secret>
----

==== Tableflow API Keys

[source,bash]
----
# Create API key for Tableflow access
confluent api-key create --resource tableflow --description "Workshop API Key for Tableflow"

# Store the Tableflow API key and secret
# Tableflow API Key: <your-tableflow-api-key>
# Tableflow API Secret: <your-tableflow-api-secret>

# Use the Tableflow API key
# export TABLEFLOW_API_KEY=<your-tableflow-api-key>
# export TABLEFLOW_API_SECRET=<your-tableflow-api-secret>

# Test Tableflow access by listing topics (should be empty initially)
confluent tableflow topic list
----

==== Store API Keys in Environment File

[source,bash]
----
# Navigate to the scripts directory
cd ./scripts/kafka

# Copy the example environment file
cp .env.example .env

# Edit the .env file with your actual API keys:
# export KAFKA_API_KEY="your-kafka-api-key"
# export KAFKA_API_SECRET="your-kafka-api-secret"
# export SCHEMA_REGISTRY_API_KEY="your-sr-api-key"
# export SCHEMA_REGISTRY_API_SECRET="your-sr-api-secret"
# export TABLEFLOW_API_KEY="your-tableflow-api-key"
# export TABLEFLOW_API_SECRET="your-tableflow-api-secret"

# Load the environment variables
source .env
----

=== Validate Cluster Connectivity

[source,bash]
----
# Test cluster connectivity by listing topics (should be empty initially)
confluent kafka topic list

# If successful, you should see an empty list or system topics
----

== ‚úÖ Validation Checklist

Before proceeding to the next section, ensure:

- [ ] VSCode Confluent Extension is installed and working
- [ ] Confluent CLI is installed (version 3.x+)
- [ ] DuckDB is installed and accessible
- [ ] Successfully logged into Confluent Cloud
- [ ] Workshop environment created and active
- [ ] Basic Kafka cluster created and running
- [ ] Kafka cluster API keys generated and configured
- [ ] Schema Registry API keys generated and configured
- [ ] Tableflow API keys generated and configured
- [ ] API keys stored in `.env` file in `scripts/kafka/` directory
- [ ] Cluster connectivity validated

== üîß Key Deliverables

At the end of this section, you should have:

* **Working Confluent Cloud environment** with proper authentication
* **Authenticated CLI session** with saved context
* **Basic Kafka cluster** ready for topic creation and data streaming
* **API keys configured** for programmatic access

== üö® Troubleshooting

=== Common Issues

**CLI Login Issues**::
If browser doesn't open automatically:
[source,bash]
----
# Try manual login with --no-browser flag
confluent login --save --no-browser
# Follow the provided URL manually
----

**Cluster Creation Fails**::
Check your account limits:
[source,bash]
----
# Check service quotas
confluent service-quota list organization
----

**API Key Issues**::
If API key creation fails:
[source,bash]
----
# List existing API keys
confluent api-key list
# Delete unused keys if you hit the limit
confluent api-key delete <key-id>
----

== üìö Additional Resources

* https://docs.confluent.io/confluent-cli/current/install.html[Confluent CLI Installation Guide]
* https://docs.confluent.io/cloud/current/get-started/index.html[Confluent Cloud Getting Started]
* https://docs.confluent.io/cloud/current/clusters/cluster-types.html[Kafka Cluster Types]

---

**Next**: Proceed to link:02-kafka-hands-on.adoc[] for Kafka topic management and HTTP connector setup.
